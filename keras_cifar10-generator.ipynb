{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "keras-cifar10.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python391jvsc74a57bd063fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d",
      "display_name": "Python 3.9.1 64-bit"
    },
    "accelerator": "GPU",
    "metadata": {
      "interpreter": {
        "hash": "63fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d"
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JVqy8wi_uqbW"
      },
      "source": [
        "# Convolution Network\n",
        "\n",
        "Convolution Neural Networks (CNN) have been very successful especially when modeling images. In this notebook we introduce CNNs and use Keras to learn the CIFAR10 data set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CAWSu-88vYRO"
      },
      "source": [
        "### Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Enabling eager execution\n",
            "INFO:tensorflow:Enabling v2 tensorshape\n",
            "INFO:tensorflow:Enabling resource variables\n",
            "INFO:tensorflow:Enabling tensor equality\n",
            "INFO:tensorflow:Enabling control flow v2\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf \n",
        "import numpy as np \n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras import models,layers\n",
        "from tensorflow.keras.utils import Sequence\n",
        "#from tensorflow.python.keras.utils import data_utils\n",
        "import math\n",
        "import os\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n"
          ]
        }
      ],
      "source": [
        "train_dir=\"C:/Users/hikma/Downloads/cifar10-pngs-in-folders/cifar10/cifar10/train\"\n",
        "test_dir=\"C:/Users/hikma/Downloads/cifar10-pngs-in-folders/cifar10/cifar10/test\"\n",
        "\n",
        "cats=os.listdir(train_dir)\n",
        "print(cats)\n",
        "train_figs=np.array([ list(map((train_dir+\"/\"+cats[i]+\"/\").__add__,os.listdir(train_dir+\"/\"+cats[i]))) for i in range(0,10)])\n",
        "test_figs=np.array([ list(map((test_dir+\"/\"+cats[i]+\"/\").__add__,os.listdir(test_dir+\"/\"+cats[i]))) for i in range(0,10)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(50000,)\n"
          ]
        }
      ],
      "source": [
        "test_figs=np.transpose(test_figs)\n",
        "test_figs=np.hstack(test_figs)\n",
        "train_figs=np.transpose(train_figs)\n",
        "train_figs=np.hstack(train_figs)\n",
        "print(train_figs.shape)\n",
        "train_labels=np.array([ i%10 for i in range(0,len(train_figs))])\n",
        "test_labels=np.array([ i%10 for i in range(0,len(test_figs))])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "#class CIFAR10Sequence(data_utils.Sequence):\n",
        "class CIFAR10Sequence(Sequence):\n",
        "    def __init__(self, x_set, y_set, batch_size):\n",
        "        self.x, self.y = x_set, y_set\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def __len__(self):\n",
        "        return math.ceil(len(self.x) / self.batch_size)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        batch_x = self.x[idx * self.batch_size:(idx + 1) *\n",
        "        self.batch_size]\n",
        "        batch_y = self.y[idx * self.batch_size:(idx + 1) *\n",
        "        self.batch_size]\n",
        "\n",
        "        return np.array([plt.imread(filename) for filename in batch_x]),batch_y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(128, 32, 32, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "batch_size=128\n",
        "train_dataset=CIFAR10Sequence(train_figs,train_labels,batch_size)\n",
        "test_dataset=CIFAR10Sequence(test_figs,test_labels,batch_size)\n",
        "(f,l)=train_dataset.__getitem__(0)\n",
        "f.shape"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lClgW_GYH42o"
      },
      "source": [
        "def createModel():\n",
        "    model = models.Sequential()\n",
        "    model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
        "    model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "    model.add(layers.Conv2D(64, (3, 3),  activation='relu'))\n",
        "    model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "    model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "    model.add(layers.Flatten())\n",
        "\n",
        "    model.add(layers.Dense(64, activation='relu'))\n",
        "    model.add(layers.Dense(10,activation='softmax'))\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rbrJYUhSEu_z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8c4ccda7-c59e-4a6a-c9cb-36a1eabdbf11"
      },
      "source": [
        "model=createModel()\n",
        "#tf.keras.utils.plot_model(model,show_shapes=True)\n",
        "model.summary()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d (Conv2D)              (None, 30, 30, 32)        896       \n_________________________________________________________________\nmax_pooling2d (MaxPooling2D) (None, 15, 15, 32)        0         \n_________________________________________________________________\nconv2d_1 (Conv2D)            (None, 13, 13, 64)        18496     \n_________________________________________________________________\nmax_pooling2d_1 (MaxPooling2 (None, 6, 6, 64)          0         \n_________________________________________________________________\nconv2d_2 (Conv2D)            (None, 4, 4, 64)          36928     \n_________________________________________________________________\nmax_pooling2d_2 (MaxPooling2 (None, 2, 2, 64)          0         \n_________________________________________________________________\nflatten (Flatten)            (None, 256)               0         \n_________________________________________________________________\ndense (Dense)                (None, 64)                16448     \n_________________________________________________________________\ndense_1 (Dense)              (None, 10)                650       \n=================================================================\nTotal params: 73,418\nTrainable params: 73,418\nNon-trainable params: 0\n_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYUxWKQr6Kjl"
      },
      "source": [
        "## Optimization\n",
        "\n",
        "Keras can use many optimization method. In this notebook we use the __Adam__ method which can be described loosely as __adaptive__ gradient descent.\n",
        "\n",
        "Also since the labels are __NOT__ in one_hot_encoding we use the \"Sparse\" version of the crossentropy loss: __SparseCategoricalCrossentropy__. Finally, if we don't specify from_logits=False then the loss function would compute softwmax before computing the loss. Since we are computing softwmax in our model already we turn this step off by specifying from_logits=False"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DbRaKJKFvug",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8607db2c-2848-40b2-8f9d-5ff7075244e5"
      },
      "source": [
        "# if we don't use softmax in the last layer, i.e. if the output of the\n",
        "# model is NOT probabilities then use from_logits=True\n",
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "#history = model.fit(img_train,label_train, batch_size=128,epochs=10, \n",
        "#                    validation_data=(img_test, label_test))\n",
        "history=model.fit(train_dataset,batch_size=batch_size,epochs=10)\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "391/391 [==============================] - 40s 89ms/step - loss: 1.8500 - accuracy: 0.3227\n",
            "Epoch 2/10\n",
            "391/391 [==============================] - 34s 88ms/step - loss: 1.3322 - accuracy: 0.5261\n",
            "Epoch 3/10\n",
            "391/391 [==============================] - 34s 88ms/step - loss: 1.1979 - accuracy: 0.5812\n",
            "Epoch 4/10\n",
            "391/391 [==============================] - 34s 88ms/step - loss: 1.0922 - accuracy: 0.6199\n",
            "Epoch 5/10\n",
            "391/391 [==============================] - 33s 85ms/step - loss: 1.0323 - accuracy: 0.6426\n",
            "Epoch 6/10\n",
            "391/391 [==============================] - 57s 147ms/step - loss: 0.9784 - accuracy: 0.6629\n",
            "Epoch 7/10\n",
            "391/391 [==============================] - 61s 155ms/step - loss: 0.9169 - accuracy: 0.6847\n",
            "Epoch 8/10\n",
            "391/391 [==============================] - 57s 145ms/step - loss: 0.8764 - accuracy: 0.6958\n",
            "Epoch 9/10\n",
            "391/391 [==============================] - 35s 89ms/step - loss: 0.8401 - accuracy: 0.7113\n",
            "Epoch 10/10\n",
            "391/391 [==============================] - 34s 88ms/step - loss: 0.8090 - accuracy: 0.7206\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLe-ndQpqbPj"
      },
      "source": [
        "### Testing the Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WuvWaksuYAKi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99665f53-05ea-46f1-9258-85f712064c62"
      },
      "source": [
        "_,test_accuracy=model.evaluate(test_dataset)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "79/79 [==============================] - 174s 2s/step - loss: 0.8974 - accuracy: 0.6936\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ]
}