{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "keras-cifar10.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "metadata": {
      "interpreter": {
        "hash": "63fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d"
      }
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hikmatfarhat-ndu/veronica-thesis/blob/master/keras_cifar10-generator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JVqy8wi_uqbW"
      },
      "source": [
        "# Convolution Network\n",
        "\n",
        "Convolution Neural Networks (CNN) have been very successful especially when modeling images. In this notebook we introduce CNNs and use Keras to learn the CIFAR10 data set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "atqyNtEcPNpw"
      },
      "source": [
        "%%bash\n",
        "fileid=\"1p8-24XAnVxRZCASVhHZobraWT8HaRqyP&export=download\" \n",
        "filename=\"cifar10.zip\"\n",
        "curl -L -c cookies.txt 'https://docs.google.com/uc?export=download&id='$fileid | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1/p' > confirm.txt\n",
        "\n",
        "curl -L -b cookies.txt -o $filename 'https://docs.google.com/uc?export=download&id='$fileid'&confirm='$(<confirm.txt)\n",
        "\n",
        "rm -f confirm.txt cookies.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zWAe-8iVyGd8"
      },
      "source": [
        "!unzip -q cifar10.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CAWSu-88vYRO"
      },
      "source": [
        "### Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "veLEHh46PKO_"
      },
      "source": [
        "import tensorflow as tf \n",
        "import numpy as np \n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras import models,layers\n",
        "from tensorflow.keras.utils import Sequence\n",
        "#from tensorflow.python.keras.utils import data_utils\n",
        "import math\n",
        "import os\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JHq24I3TPKPC"
      },
      "source": [
        "train_dir=\"/content/cifar10/cifar10/train\"\n",
        "test_dir=\"/content/cifar10/cifar10/test\"\n",
        "\n",
        "cats=os.listdir(train_dir)\n",
        "print(cats)\n",
        "train_figs=np.array([ list(map((train_dir+\"/\"+cats[i]+\"/\").__add__,os.listdir(train_dir+\"/\"+cats[i]))) for i in range(0,10)])\n",
        "test_figs=np.array([ list(map((test_dir+\"/\"+cats[i]+\"/\").__add__,os.listdir(test_dir+\"/\"+cats[i]))) for i in range(0,10)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O0hje6hIPKPD"
      },
      "source": [
        "test_figs=np.transpose(test_figs)\n",
        "test_figs=np.hstack(test_figs)\n",
        "train_figs=np.transpose(train_figs)\n",
        "train_figs=np.hstack(train_figs)\n",
        "print(train_figs.shape)\n",
        "train_labels=np.array([ i%10 for i in range(0,len(train_figs))])\n",
        "test_labels=np.array([ i%10 for i in range(0,len(test_figs))])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSR5UsL5PKPE"
      },
      "source": [
        "#class CIFAR10Sequence(data_utils.Sequence):\n",
        "class CIFAR10Sequence(Sequence):\n",
        "    def __init__(self, x_set, y_set, batch_size):\n",
        "        self.x, self.y = x_set, y_set\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def __len__(self):\n",
        "        return math.ceil(len(self.x) / self.batch_size)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        batch_x = self.x[idx * self.batch_size:(idx + 1) *\n",
        "        self.batch_size]\n",
        "        batch_y = self.y[idx * self.batch_size:(idx + 1) *\n",
        "        self.batch_size]\n",
        "\n",
        "        return np.array([plt.imread(filename) for filename in batch_x]),batch_y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eqYsPEGWPKPF"
      },
      "source": [
        "batch_size=128\n",
        "train_dataset=CIFAR10Sequence(train_figs,train_labels,batch_size)\n",
        "test_dataset=CIFAR10Sequence(test_figs,test_labels,batch_size)\n",
        "(f,l)=train_dataset.__getitem__(0)\n",
        "f.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lClgW_GYH42o"
      },
      "source": [
        "def createModel():\n",
        "    model = models.Sequential()\n",
        "    model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
        "    model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "    model.add(layers.Conv2D(64, (3, 3),  activation='relu'))\n",
        "    model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "    model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "    model.add(layers.Flatten())\n",
        "\n",
        "    model.add(layers.Dense(64, activation='relu'))\n",
        "    model.add(layers.Dense(10,activation='softmax'))\n",
        "    \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rbrJYUhSEu_z"
      },
      "source": [
        "model=createModel()\n",
        "#tf.keras.utils.plot_model(model,show_shapes=True)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYUxWKQr6Kjl"
      },
      "source": [
        "## Optimization\n",
        "\n",
        "Keras can use many optimization method. In this notebook we use the __Adam__ method which can be described loosely as __adaptive__ gradient descent.\n",
        "\n",
        "Also since the labels are __NOT__ in one_hot_encoding we use the \"Sparse\" version of the crossentropy loss: __SparseCategoricalCrossentropy__. Finally, if we don't specify from_logits=False then the loss function would compute softwmax before computing the loss. Since we are computing softwmax in our model already we turn this step off by specifying from_logits=False"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DbRaKJKFvug"
      },
      "source": [
        "# if we don't use softmax in the last layer, i.e. if the output of the\n",
        "# model is NOT probabilities then use from_logits=True\n",
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "#history = model.fit(img_train,label_train, batch_size=128,epochs=10, \n",
        "#                    validation_data=(img_test, label_test))\n",
        "history=model.fit(train_dataset,batch_size=batch_size,epochs=10)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLe-ndQpqbPj"
      },
      "source": [
        "### Testing the Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WuvWaksuYAKi"
      },
      "source": [
        "_,test_accuracy=model.evaluate(test_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2PPy_mPPKPK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}